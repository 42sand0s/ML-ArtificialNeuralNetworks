# ML-ArtificialNeuralNetworks

Implementation of Artificial Neural Network with one and two hidden layers from scratch using Python. The only libraries used are numpy, pandas, warnings and matplotlib.

The entire dataset was divided into 70:30 ratio for training and testing respectively. Input feature vectors were normalised for better and faster results.
ReLU is the Activation Function used for one layer ANN and those for two layers are ReLU and Softmax.

Plot for Accuracy and Loss are plotted against number of iterations.

Accuracy on testing data sample for one layer ANN:  82.0%

Accuracy on testing data sample for two layers ANN: 82.667%
